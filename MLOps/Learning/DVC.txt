Data Version Control (DVC) [s3, Blog, GCP storage]
  1. Large Files
  2. Verioning
  3. Cheap
  4. Durable
Data Versioning : [.dvc file and dvc config created when dvc commands runs] These two files are pointed to the actual dataset which is stored in Cloud storage.
We cannot use GIT for Data Versioning because:
  1. Large size (GB, or more ~TB).
  2. Slow on large files.
  3. Cost inefficient for ORG.
  If we use GIT for this, GIT hosted on a VM will use local storage of that VM. Large files on GIT ie on local Disc, which is not      cost effective.

GIT hosts:
  1. python scripts
  2. api scripts
  3. .dvc file
  4. dvc config

CSV files are stored in remote location or storage.
Feature Engineering is done on CSV files.

For a Data Scientist:
1. Data (DVC) : An S3 bucket, Azure Blob or GCP storage. Remote cloud storage.
2. Algorithm and Training Process.
  Python Scripts (Stored in GIT repo) by DS or ML Engg: for Auditing, Versioning, RBAC for control.
  GIT implemnent Versiong for above scripts.

Commands.
  1. dvc add
  2. dvc push
  3. dvc pull

(Pyhton is a pr-requiste for this)
>>python3 -m venv .venv
>>python3 -m pip install dvc
>>git init
>>dvc init

<wine-sample>.csv.dvc (.dvc)file is created which holds the meta data of actual csv files. This .dvc file is uploaded to git repo. 
And actual sample data is stored in remote storage.

>>dvc add <wine-sample>.csv
  When ever you have changed the .csv file you need to run >>dvc add file-name. This will update the .dvc file for updated csv file,   which can later pushed to git repo.

To Push remote storage (S3):
1. Create a s3 bucket.
2. Add this remote bucket. 
  >>aws configure ( to add s3 to push dataset)
  >>python3 -m pip install dvc_s3 (a dependency for s3)
  >>dvc add -d wineremote s3://<name of s3 bucket>
  >>dvc push



Feature engineering is the process of using domain knowledge to extract, transform, and select relevant variables (features) from raw data to improve machine learning model performance. It involves techniques like handling missing values, encoding categorical data, scaling numerical data, and creating new features to better represent the underlying patterns.
